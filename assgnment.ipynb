{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7f9b07-1d8c-4e79-87c3-e83bc71de75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-HoYiTik\\Assgnment\\data\\characters\n",
      "OUTPUT_ROOT: C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-HoYiTik\\Assgnment\\processed_data\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Imports & paths (edit nothing if your paths are as stated)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# === EDIT IF NEEDED ===\n",
    "DATA_ROOT = Path(r\"C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-HoYiTik\\Assgnment\\data\\characters\")\n",
    "NOTEBOOK_ROOT = Path(r\"C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-HoYiTik\\Assgnment\")\n",
    "OUTPUT_ROOT = NOTEBOOK_ROOT / \"processed_data\"            # will be created\n",
    "MODELS_DIR = NOTEBOOK_ROOT / \"saved_models\"\n",
    "REPORTS_DIR = NOTEBOOK_ROOT / \"reports\"\n",
    "# =======================\n",
    "\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"OUTPUT_ROOT:\", OUTPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35230810-67c7-491b-8b93-820e6073a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration & helper augmentation functions\n",
    "# General config (tune these to your hardware)\n",
    "IMG_SIZE = (64, 64)         # resize images to 64x64 (grayscale)\n",
    "TARGET_PER_CLASS = 200      # create at least this many training samples per class\n",
    "TRAIN_SAMPLES_PER_CLASS = 40  # the first 40 samples are training seeds (per assignment)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12                 # increase when you have more time/GPU\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "FORCE_NUM_CLASSES = None    # set to 13065 to force output neurons (optional). If None uses actual classes.\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Image augmentation helpers using PIL (keeps dependencies minimal)\n",
    "def random_augment_pil(img: Image.Image):\n",
    "    \"\"\"Apply random augmentation to a PIL image and return a new PIL image.\"\"\"\n",
    "    # Random rotation\n",
    "    angle = random.uniform(-20, 20)\n",
    "    img = img.rotate(angle, resample=Image.BILINEAR, expand=False, fillcolor=255)\n",
    "\n",
    "    # Random affine (translate / shear)\n",
    "    max_shift = 0.12  # fraction of width/height\n",
    "    tx = random.uniform(-max_shift, max_shift) * img.width\n",
    "    ty = random.uniform(-max_shift, max_shift) * img.height\n",
    "    shear = random.uniform(-8, 8)\n",
    "    img = img.transform(\n",
    "        img.size,\n",
    "        Image.AFFINE,\n",
    "        (1, -shear/100.0, tx, shear/100.0, 1, ty),\n",
    "        resample=Image.BILINEAR,\n",
    "        fillcolor=255\n",
    "    )\n",
    "\n",
    "    # Random scale / crop then pad back (zoom)\n",
    "    zoom = random.uniform(0.85, 1.15)\n",
    "    new_w = int(img.width * zoom)\n",
    "    new_h = int(img.height * zoom)\n",
    "    img = img.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "    # center-crop or pad to original size\n",
    "    img = ImageOps.fit(img, (img.width, img.height), method=Image.BILINEAR)\n",
    "    img = ImageOps.fit(img, IMG_SIZE, method=Image.BILINEAR)\n",
    "\n",
    "    # Random contrast/brightness\n",
    "    if random.random() < 0.5:\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        img = enhancer.enhance(random.uniform(0.8, 1.2))\n",
    "    if random.random() < 0.5:\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        img = enhancer.enhance(random.uniform(0.85, 1.15))\n",
    "\n",
    "    # final convert to 'L' (grayscale)\n",
    "    img = img.convert('L')\n",
    "    return img\n",
    "\n",
    "def load_image_as_pil(path):\n",
    "    img = Image.open(path).convert('L')\n",
    "    # ensure consistent canvas and size\n",
    "    img = ImageOps.invert(img) if np.mean(img) < 128 else img  # try to make background light if needed\n",
    "    img = ImageOps.fit(img, IMG_SIZE, method=Image.BILINEAR)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0f542c-023c-46a6-997d-51d49429b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13065 class folders under DATA_ROOT.\n",
      "Preparing train/test split and augmentation. This can take time depending on dataset size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 13065/13065 [38:29<00:00,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Build train/test directories and perform augmentation until >= TARGET_PER_CLASS per class\n",
    "# Directory layout created:\n",
    "# OUTPUT_ROOT/train/<class_name>/*.png\n",
    "# OUTPUT_ROOT/test/<class_name>/*.png\n",
    "\n",
    "train_dir = OUTPUT_ROOT / \"train\"\n",
    "test_dir = OUTPUT_ROOT / \"test\"\n",
    "# clean previous processed dirs if exist (uncomment to force fresh)\n",
    "# shutil.rmtree(OUTPUT_ROOT, ignore_errors=True)\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# discover class folders\n",
    "class_dirs = [p for p in sorted(DATA_ROOT.iterdir()) if p.is_dir()]\n",
    "print(f\"Found {len(class_dirs)} class folders under DATA_ROOT.\")\n",
    "\n",
    "classes = []\n",
    "for cpath in class_dirs:\n",
    "    class_name = cpath.name\n",
    "    classes.append(class_name)\n",
    "\n",
    "# If no class directories (maybe files in root), try a fallback: treat files named like \"<char>_xxx.png\"\n",
    "if not classes:\n",
    "    images = [p for p in sorted(DATA_ROOT.iterdir()) if p.is_file()]\n",
    "    # group by prefix before underscore\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for im in images:\n",
    "        name = im.name\n",
    "        key = name.split('_')[0]\n",
    "        groups[key].append(im)\n",
    "    classes = sorted(groups.keys())\n",
    "    class_dirs = []\n",
    "    for k in classes:\n",
    "        # create temporary list of files\n",
    "        class_dirs.append((k, groups[k]))\n",
    "\n",
    "print(\"Preparing train/test split and augmentation. This can take time depending on dataset size.\")\n",
    "\n",
    "# If classes were normal Path objects:\n",
    "if all(isinstance(p, Path) for p in class_dirs):\n",
    "    for p in tqdm(class_dirs, desc=\"Processing classes\"):\n",
    "        class_name = p.name\n",
    "        files = sorted([f for f in p.iterdir() if f.suffix.lower() in ('.png','.jpg','.jpeg')])\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "        # take first 40 as seeds for training (or fewer if not enough)\n",
    "        seed_train = files[:TRAIN_SAMPLES_PER_CLASS]\n",
    "        seed_test = files[TRAIN_SAMPLES_PER_CLASS:]\n",
    "        # create dirs\n",
    "        (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        (test_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        # copy test files\n",
    "        for t in seed_test:\n",
    "            dst = test_dir / class_name / t.name\n",
    "            if not dst.exists():\n",
    "                shutil.copy2(t, dst)\n",
    "        # copy seeds (and augment to reach TARGET_PER_CLASS)\n",
    "        # copy original seeds first\n",
    "        for idx, s in enumerate(seed_train):\n",
    "            dst = train_dir / class_name / f\"seed_{idx:03d}{s.suffix}\"\n",
    "            if not dst.exists():\n",
    "                shutil.copy2(s, dst)\n",
    "        # count existing\n",
    "        existing = list((train_dir / class_name).glob(\"*\"))\n",
    "        # augment from seeds randomly until we reach target\n",
    "        seed_images = [load_image_as_pil(p) for p in seed_train] if seed_train else []\n",
    "        # if no seed images but there are test images, we will use any test image to seed (unlikely)\n",
    "        if not seed_images and seed_test:\n",
    "            seed_images = [load_image_as_pil(seed_test[0])]\n",
    "        i = len(existing)\n",
    "        attempts = 0\n",
    "        while i < TARGET_PER_CLASS and attempts < TARGET_PER_CLASS * 20:\n",
    "            attempts += 1\n",
    "            if not seed_images:\n",
    "                break\n",
    "            src = random.choice(seed_images)\n",
    "            aug = random_augment_pil(src)\n",
    "            # save\n",
    "            out_name = f\"aug_{i:04d}.png\"\n",
    "            out_path = train_dir / class_name / out_name\n",
    "            aug.save(out_path)\n",
    "            i += 1\n",
    "        if i < TARGET_PER_CLASS:\n",
    "            print(f\"Warning: class {class_name} only reached {i} training images (target {TARGET_PER_CLASS}).\")\n",
    "else:\n",
    "    # fallback grouping case (if data was files with prefixes)\n",
    "    for class_name, files in tqdm(class_dirs, desc=\"Processing grouping fallback\"):\n",
    "        (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        (test_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        files = sorted(files)\n",
    "        seed_train = files[:TRAIN_SAMPLES_PER_CLASS]\n",
    "        seed_test = files[TRAIN_SAMPLES_PER_CLASS:]\n",
    "        for t in seed_test:\n",
    "            shutil.copy2(t, test_dir / class_name / t.name)\n",
    "        for idx, s in enumerate(seed_train):\n",
    "            dst = train_dir / class_name / f\"seed_{idx:03d}{s.suffix}\"\n",
    "            if not dst.exists():\n",
    "                shutil.copy2(s, dst)\n",
    "        seed_images = [load_image_as_pil(p) for p in seed_train] if seed_train else []\n",
    "        i = len(list((train_dir / class_name).glob(\"*\")))\n",
    "        attempts = 0\n",
    "        while i < TARGET_PER_CLASS and attempts < TARGET_PER_CLASS * 20:\n",
    "            attempts += 1\n",
    "            if not seed_images:\n",
    "                break\n",
    "            src = random.choice(seed_images)\n",
    "            aug = random_augment_pil(src)\n",
    "            out_name = f\"aug_{i:04d}.png\"\n",
    "            out_path = train_dir / class_name / out_name\n",
    "            aug.save(out_path)\n",
    "            i += 1\n",
    "        if i < TARGET_PER_CLASS:\n",
    "            print(f\"Warning: class {class_name} only reached {i} training images (target {TARGET_PER_CLASS}).\")\n",
    "\n",
    "print(\"Finished data processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a55eda-c699-4760-bf45-98eb5ac54afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2613000 files belonging to 13065 classes.\n",
      "Found 162550 files belonging to 13065 classes.\n",
      "Detected classes: 13065\n",
      "Batch shape: (128, 64, 64, 1) (128, 13065)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Build tf.data datasets from processed directories\n",
    "# Determine classes and num_classes\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(\"Detected classes:\", NUM_CLASSES)\n",
    "\n",
    "if FORCE_NUM_CLASSES is not None:\n",
    "    print(f\"Forcing num classes to {FORCE_NUM_CLASSES} (assignment requirement).\")\n",
    "    NUM_CLASSES = FORCE_NUM_CLASSES\n",
    "\n",
    "# normalize and prepare pipeline\n",
    "def normalize_img(image, label):\n",
    "    # convert uint8 [0,255] -> float32 [0,1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "train_ds = train_ds.map(normalize_img, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.map(normalize_img, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "# small check: one batch\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"Batch shape:\", images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d185409-0bf6-4a70-bc43-1101532ca762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 64, 1) Num classes: 13065\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Model definitions: Baseline MLP, small CNN, deeper CNN\n",
    "def build_mlp(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_small_cnn(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_deeper_cnn(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Input shape (grayscale)\n",
    "input_shape = IMG_SIZE + (1,)\n",
    "print(\"Input shape:\", input_shape, \"Num classes:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58adab-1130-40d8-9038-96cddeeab5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========= Training: mlp_baseline =========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13065</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,702,345</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13065\u001b[0m)          │     \u001b[38;5;34m6,702,345\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,422,473</span> (43.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,422,473\u001b[0m (43.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,422,473</span> (43.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,422,473\u001b[0m (43.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m20415/20415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 5.6677e-05 - loss: 9.4844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20415/20415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1956s\u001b[0m 96ms/step - accuracy: 4.8220e-05 - loss: 9.4834 - val_accuracy: 6.7671e-05 - val_loss: 9.4783 - learning_rate: 0.0010\n",
      "Epoch 2/12\n",
      "\u001b[1m12054/20415\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m11:51\u001b[0m 85ms/step - accuracy: 4.0799e-05 - loss: 9.4813"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Training helper that trains a model and returns history + evaluation\n",
    "def train_and_evaluate(model_fn, model_name, epochs=EPOCHS):\n",
    "    print(\"\\n\\n========= Training:\", model_name, \"=========\")\n",
    "    model = model_fn(input_shape, NUM_CLASSES)\n",
    "    model.summary()\n",
    "\n",
    "    ckpt_path = MODELS_DIR / f\"{model_name}.h5\"\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(ckpt_path), monitor='val_accuracy', save_best_only=True, save_weights_only=False),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=test_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Evaluate best saved model\n",
    "    model.load_weights(str(ckpt_path))\n",
    "    loss, acc = model.evaluate(test_ds, verbose=2)\n",
    "    print(f\"{model_name} test accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Return model, history, and numeric accuracy\n",
    "    return model, history, acc\n",
    "\n",
    "# Train models sequentially (be careful: this will take time)\n",
    "results = []\n",
    "models_to_run = [\n",
    "    (build_mlp, \"mlp_baseline\"),\n",
    "    (build_small_cnn, \"small_cnn\"),\n",
    "    (build_deeper_cnn, \"deeper_cnn\")\n",
    "]\n",
    "\n",
    "for fn, name in models_to_run:\n",
    "    model, hist, acc = train_and_evaluate(fn, name)\n",
    "    results.append({\"model\": name, \"accuracy\": float(acc)})\n",
    "    # Save history CSV\n",
    "    hist_df = pd.DataFrame(hist.history)\n",
    "    hist_df.to_csv(REPORTS_DIR / f\"{name}_history.csv\", index=False)\n",
    "\n",
    "# Save results summary\n",
    "pd.DataFrame(results).to_csv(REPORTS_DIR / \"model_accuracies.csv\", index=False)\n",
    "print(\"All done. Summaries at:\", REPORTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6bbe5-13a6-4e14-85e3-97d26cadf7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Classification report on test set (samples only)\n",
    "# This will compute predicted labels and show overall accuracy and a short classification report\n",
    "print(\"Building predictions for test dataset ... (this may take time for large class counts)\")\n",
    "\n",
    "# collect true labels and predictions (may be large)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "file_paths = []\n",
    "\n",
    "# iterate through test_ds dataset but we need the original integer labels; to get them easier,\n",
    "# reload a non-one-hot dataset with label_mode='int' and no batching\n",
    "test_ds_int = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=1,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# load best model (choose deepest as example)\n",
    "best_model_path = MODELS_DIR / \"deeper_cnn.h5\"\n",
    "if best_model_path.exists():\n",
    "    best_model = keras.models.load_model(best_model_path)\n",
    "else:\n",
    "    # fallback to the last trained model\n",
    "    best_model = model\n",
    "\n",
    "for img, label in tqdm(test_ds_int, desc=\"Predicting\"):\n",
    "    img_norm = tf.cast(img, tf.float32) / 255.0\n",
    "    preds = best_model.predict(img_norm, verbose=0)\n",
    "    pred_label = np.argmax(preds, axis=-1)[0]\n",
    "    true_label = int(label.numpy()[0])\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "# global accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Test accuracy (computed):\", acc)\n",
    "\n",
    "# print a short classification report for up to first 30 classes to avoid huge outputs\n",
    "n_display = min(30, NUM_CLASSES)\n",
    "target_names = class_names[:n_display]\n",
    "report = classification_report(y_true, y_pred, labels=list(range(n_display)), target_names=target_names, zero_division=0)\n",
    "print(\"Classification report (first classes):\\n\", report)\n",
    "\n",
    "# Save predictions CSV\n",
    "pred_df = pd.DataFrame({\"true\": y_true, \"pred\": y_pred})\n",
    "pred_df.to_csv(REPORTS_DIR / \"test_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to:\", REPORTS_DIR / \"test_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
